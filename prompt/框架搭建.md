我已经完成了环境交互层的初步搭建，包含文档、核心代码和测试脚本。

### 1. 文档 (`docs/01_environment_design.md`)
编写了详细的设计文档，解释了为何选择 **ADB + Scrcpy + Window Capture + Minitouch** 的组合：
-   **ADB**: 设备管理基石。
-   **Scrcpy**: 提供低延迟画面投射。
-   **Window Capture**: 通过 Windows API 高效获取画面（比 ADB 截图快几十倍）。
-   **Minitouch**: 提供毫秒级触控响应（比 ADB Input 快得多）。

### 2. 核心代码 (`main/env/`)
我创建了模块化的 Python 包，包含以下组件：

-   [adb_wrapper.py](file:///d:\Project\auto-wzry\main\env\adb_wrapper.py):
    -   封装 `adb` 命令，支持获取设备列表、执行 Shell 命令、端口转发等。
-   [scrcpy_wrapper.py](file:///d:\Project\auto-wzry\main\env\scrcpy_wrapper.py):
    -   负责启动 Scrcpy 进程，支持自定义窗口标题（这对后续精准截图至关重要）和分辨率/比特率设置。
-   [window_capture.py](file:///d:\Project\auto-wzry\main\env\window_capture.py):
    -   使用 `pywin32` 查找并截取指定标题的窗口。这是 AI 获取视觉输入的关键。
-   [minitouch_wrapper.py](file:///d:\Project\auto-wzry\main\env\minitouch_wrapper.py):
    -   基于 `pyminitouch` 封装，提供 `tap` (点击) 和 `swipe` (滑动) 接口。

### 3. 测试脚本 (`main/test_env.py`)
创建了一个集成测试脚本 [test_env.py](file:///d:\Project\auto-wzry\main\test_env.py)。
-   它会尝试连接第一个 Android 设备。
-   启动 Scrcpy 并指定一个唯一的测试窗口标题。
-   尝试截取 5 帧画面并打印尺寸，验证截图功能是否正常。
-   最后自动清理进程。

### 下一步建议
现在环境层已经就绪，我们可以开始考虑 **数据处理 pipeline**：
1.  如何将截取的图片 (`numpy array`) 转换为模型需要的 Tensor。
2.  如何定义“状态” (State) 和“动作” (Action)。
3.  是否需要先写一个简单的脚本来采集一些训练数据（比如手动玩一局，记录图片和操作）？这可以用来验证我们的数据流是否通畅。