# 06. 深度解剖：一张图片是如何变成操作指令的？

在前面的文档中，我们介绍了如何训练模型。但你是否好奇，当模型“看”到一张游戏画面时，它的脑子里到底发生了什么？

本文将以一张实际的游戏截图为例，带您走进 AI 的“大脑”，追踪数据流动的全过程。

---

## 1. 我们的模型架构概览

我们采用的是经典的 **Actor-Critic (演员-评论家)** 架构，结合了 **CNN (卷积神经网络)** 作为视觉中枢。

可以把它想象成一个由三个部门组成的公司：
1.  **视觉部 (CNN Encoder)**：负责“看”。把复杂的图像像素转化为精简的“情报”。
2.  **决策部 (Actor)**：负责“做”。根据情报，决定手指该怎么动。
3.  **评估部 (Critic)**：负责“评”。根据情报，判断现在的局势好不好。

### 代码对应 (`main/model.py`)
*   `CNNEncoder`: 视觉部 (基于 ResNet18)
*   `ActorCritic.actor_mean`: 决策部 (全连接层)
*   `ActorCritic.critic`: 评估部 (全连接层)

---

## 2. 全流程演示：一张图片的奇幻漂流

假设现在游戏里发生了一次团战，屏幕上显示了英雄、小兵和敌方。我们将追踪这一帧画面 (`frame_t`) 是如何一步步变成代码指令的。

### 第一步：眼睛 (预处理 Preprocessing)

*   **输入**: 手机屏幕原始截图 (比如 2340 x 1080 像素)。
*   **动作**: 
    1.  **缩放 (Resize)**: 这种高清大图对 AI 来说太大了，处理不动。我们会把它强行压缩成 `224 x 224` 的正方形小图。虽然人眼看糊了，但 AI 依然能分辨出哪里是人，哪里是塔。
    2.  **归一化 (Normalization)**: 把像素值从 0-255 (整数) 变成 0.0-1.0 (浮点数)，方便数学计算。
    3.  **维度变换 (To Tensor)**: 变成 PyTorch 能理解的格式 `[Batch, Channel, Height, Width]` -> `[1, 3, 224, 224]`。

> **结果**: 一个 `(1, 3, 224, 224)` 的数字矩阵。

### 第二步：视觉中枢 (Feature Extraction)

这是最神奇的一步。这个矩阵被送入 `CNNEncoder` (基于 ResNet18)。

*   **卷积层 (Convolution)**: 就像无数个小放大镜，在图片上扫描。
    *   浅层卷积看到的是**线条、颜色斑点**（比如红色的血条、绿色的草地）。
    *   深层卷积看到的是**形状、物体**（比如“这里有个圆形的英雄底座”，“那里是防御塔的轮廓”）。
*   **压缩与抽象**:
    *   ResNet18 把 `224x224` 的大图，层层浓缩。
    *   最后，它输出一个 **512维的向量 (Vector)**。

> **关键理解**: 
> 此时，图片不再是图片了，而是一串“概念代码”。
> 这 512 个数字可能代表了：
> *   第 1 个数: "视野里有敌人吗？" (0.9)
> *   第 2 个数: "我的血量健康吗？" (0.2)
> *   ...
> *   第 512 个数: "距离泉水多远？" (0.5)
> *(注：神经网络内部特征是不可解释的，这里只是比喻)*

### 💡 深度思考：只有 512 个因素够用吗？

**Q：既然最后只输出了 512 个数字，那是不是说明 AI 做决策时，只考虑了 512 个因素？**

**A：是的，您的直觉非常敏锐！** 这就是深度学习中著名的**“信息瓶颈” (Information Bottleneck)** 概念。

1.  **极致压缩**：
    *   原始图片有 $224 \times 224 \times 3 \approx 150,000$ 个像素点。
    *   我们强制模型把这 15 万个点的信息，浓缩进 **512** 个数字里。
    *   这就好比要求你用“512 个字”概括整场比赛的局势。你不可能记录每一根草的位置，只能**提炼最核心的情报**（如：谁在哪、血量多少、有没有大招）。

2.  **抽象的力量**：
    *   但这 512 个“因素”并不是简单的线性指标。
    *   神经网络会自动学习将**复合信息**编码进去。例如，其中某一个数值可能不仅仅代表“有敌人”，而是代表 *“敌方刺客正在从左侧草丛切入且我方技能全黑”* 这样高度抽象的战术危机感。
    *   在王者荣耀这种级别的游戏中，512 维的特征向量通常已经足够涵盖单帧画面的关键决策信息了。

3.  **如果不构怎么办？**
    *   如果模型表现不好，我们可能会怀疑“脑容量”不够。这时可以尝试增加维度（比如改用 1024 维），但这也会导致计算量变大，训练变慢。512 是我们在性能和速度之间的一个平衡选择。

### 第三步：决策部 (Actor Head)

现在，`Actor` 拿到了这 512 个情报数据。

*   **全连接层 (Linear Layer)**: 它通过一个巨大的权重矩阵，综合考虑这 512 个因素。
    *   它可能会想：“虽然看到敌人了（因素1），但我血量很低（因素2），所以应该往回跑。”
*   **激活函数 (Sigmoid)**:
    *   神经网络输出原本是 `-infinity` 到 `+infinity` 的任意数值。
    *   但我们的手机屏幕坐标是 `(0, 0)` 到 `(1, 1)`。
    *   所以我们用 `Sigmoid` 函数把输出强行压缩到 `0.0` 到 `1.0` 之间。

> **结果**: 输出动作向量 `[x, y, is_skill, ...]`
> 比如 `[0.8, 0.2, 0.0, 0.0]`。这意味着：
> *   x=0.8, y=0.2 (屏幕右上角，可能是撤退方向)
> *   没有释放技能。

### 第四步：评估部 (Critic Head)

与此同时，`Critic` 也拿到了同样的 512 个情报数据。

*   **任务**: 它不关心怎么动，它只关心**“我现在处境如何？”**
*   **输出**: 一个单一的实数 (Value)。
    *   如果输出 `10.0`: 代表“这把稳了，优势很大”。
    *   如果输出 `-5.0`: 代表“完蛋，要被团灭了”。

> **作用**: 这个分数并不直接控制角色，但它会在训练时告诉 Actor：“你刚才那步做得对不对”。如果 Critic 觉得局势变好了，Actor 就会被奖励，反之被惩罚。

---

## 3. 总结

| 组件 | 现实类比 | 输入 | 输出 | 核心作用 |
| :--- | :--- | :--- | :--- | :--- |
| **预处理** | 戴上眼镜 | 原始截图 | 224x224 矩阵 | 格式化数据，减小计算量 |
| **CNN Encoder** | **大脑视觉区** | 224x224 矩阵 | **512维 特征向量** | **理解画面**，提取关键信息 |
| **Actor** | **运动神经** | 512维 特征向量 | **动作坐标 (x, y)** | **输出策略**，控制手操作 |
| **Critic** | **教练/解说** | 512维 特征向量 | **局势评分 (Value)** | **辅助训练**，评价当前状态好坏 |

这就是为什么我们在 `main/model.py` 中看到 `ActorCritic` 类共享了一个 `self.encoder`，然后分叉出 `self.actor_mean` 和 `self.critic` 两个头。

> **注意**：在 `__init__` 函数中，我们只是**定义组件**（买零件）。真正的连接（组装）是在 `forward` 函数中发生的。

```python
class ActorCritic(nn.Module):
    def __init__(self, ...):
        # 1. 定义视觉中枢 (零件A)
        self.encoder = CNNEncoder(...) 
        
        # 2. 定义决策部 (零件B)
        # 注意：这里只定义了它的结构是 "从 512 输入变到 action_dim 输出"
        # 并没有说它的输入一定要来自 encoder
        self.actor_mean = nn.Linear(hidden_dim, action_dim)
        
        # 3. 定义评估部 (零件C)
        self.critic = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        # 4. 在这里进行真正的“组装”和“通电”
        
        # 第一步：数据先流过 encoder
        features = self.encoder(state)  # state -> [512维特征]
        
        # 第二步：特征分流
        # 一部分流向 Actor
        mean = self.actor_mean(features) # [512维特征] -> [动作]
        
        # 另一部分流向 Critic
        value = self.critic(features)    # [512维特征] -> [评分]
        
        return mean, std, value
```

### 🔎 显微镜：数据变形记 (Data Flow Example)

我们提供了一个专门的脚本来演示这个过程。
您可以直接运行 `main/debug/inspect_model.py`，或者在 `start.bat` 菜单中选择 **[7] 模型显微镜**。

它会模拟一张随机图片，并打印出它在模型中每一步的形状变化。

#### 运行结果示例：

```text
==================================================
       🔍 AI 模型数据流显微镜 (Model Inspector)       
==================================================
正在初始化模型...

1. [输入层] 模拟游戏画面
   - 形状: torch.Size([1, 3, 224, 224])
   - 说明: 这是一个 224x224 的 RGB 图像矩阵
--------------------------------------------------
2. [视觉中枢] CNN Encoder 特征提取
   - 操作: features = model.encoder(image)
   - 输出形状: torch.Size([1, 512])
   - 前10个特征值: [ 0.82 -0.15  2.30 ...]
   - 说明: 图片被压缩成了 512 个抽象数字
--------------------------------------------------
3. [决策部] Actor Head 生成动作
   - 操作: mean = sigmoid(model.actor_mean(features))
   - 输出形状: torch.Size([1, 4])
   - 具体数值: [0.75 0.50 0.99 0.01]
   - 解读: [x坐标, y坐标, 按下概率, 预留]
--------------------------------------------------
4. [评估部] Critic Head 局势打分
   - 操作: value = model.critic(features)
   - 输出形状: torch.Size([1, 1])
   - 局势评分: 15.5000
   - 解读: 分数越高，代表当前局势越有利
--------------------------------------------------
```

## 3. 进化演示：PPO 如何修改神经元？

除了看模型如何思考，我们还提供了一个演示脚本，看看模型是如何“学习”的。
运行 `main/debug/inspect_ppo.py` 或者在菜单选择 **[8] PPO 进化实验室**。

这个实验会模拟一次 PPO 更新过程：
1.  让 AI 回忆之前的某个动作（比如乱放技能）。
2.  告诉它这个动作是好是坏（Advantage 优势值）。
3.  观察神经元权重（Weight）在反向传播后发生了微小的改变。

这就是 AI 变强的根本原因：**无数次微小的权重修正，最终汇聚成强大的直觉。**

### 手动推演

让我们给变量赋予具体的值，看看数据到底是怎么变形的：

**1. 输入 State (一张图片)**
*   **形状**: `[1, 3, 224, 224]` (Batch=1, RGB通道=3, 高=224, 宽=224)
*   **内容**: 一个巨大的数字矩阵，代表屏幕像素。

**2. 经过 Encoder (视觉中枢)**
*   **操作**: `features = self.encoder(state)`
*   **形状**: `[1, 512]`
*   **内容**: 512 个抽象特征值。
    ```python
    features = [0.82, -0.15, 2.30, ..., 0.05] # 共 512 个数
    ```

**3. 兵分两路**

*   **A路：流向 Actor (决策)**
    *   **操作**: `mean = self.actor_mean(features)`
    *   **形状**: `[1, 4]` (假设动作维度是4：x, y, is_touch, reserved)
    *   **内容**: 具体的动作指令。
        ```python
        mean = [0.75, 0.50, 0.99, 0.01] 
        # 翻译：鼠标移到屏幕(0.75, 0.50)处，并且按下(0.99接近1)
        ```

*   **B路：流向 Critic (评估)**
    *   **操作**: `value = self.critic(features)`
    *   **形状**: `[1, 1]`
    *   **内容**: 对当前局势的打分。
        ```python
        value = [15.5]
        # 翻译：当前局势非常有利！
        ```
