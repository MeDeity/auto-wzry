# 07. 变强的本质：反向传播 (Backpropagation)

在上一节的 **[8] PPO 进化实验室** 中，我们亲眼见证了神经元的权重发生了微小的改变。

你可能会问：**“电脑是怎么知道该往哪个方向改的？怎么知道该改多大？”**

这就涉及到了深度学习最核心、最迷人、也是最本质的魔法——**反向传播 (Backpropagation)**。

---

## 1. 一个直观的例子：射击训练

想象你是一个蒙着眼睛的射击手（这就是**初始状态的 AI**，一片混沌）。
你的目标是打中靶心（这就是**最优解**）。

### 第一步：盲射 (Forward Pass / 前向传播)
你凭感觉开了一枪。
*   **AI 的行为**: 输入画面 -> 经过神经网络 -> 输出动作（乱点了一个坐标）。

### 第二步：报靶 (Loss Calculation / 计算误差)
教练（**Reward / 奖励函数**）告诉你：“偏了！偏左了 10 厘米，偏上 5 厘米。”
*   **AI 的行为**: 计算 Loss (损失值)。
    *   Loss 越大，说明错得越离谱。
    *   Loss = 0，说明正中红心。

### 第三步：找原因 (Backward Pass / 反向传播)
这是最关键的一步。你需要反思：**“为什么会偏左上？”**
是因为我的手抖了？还是肩膀没沉住？还是呼吸没调整好？
*   **AI 的行为**: 计算**梯度 (Gradient)**。
    *   根据链式法则（Calculus Chain Rule），数学家发明了一种算法，可以从结果倒推回去，计算出**每一个神经元**对这次错误的“贡献度”。
    *   比如：第 3 层第 5 个神经元说：“怪我，刚才我激动了，信号发强了。”
    *   第 2 层第 1 个神经元说：“不怪我，我刚才没干坏事。”

### 第四步：微调 (Optimizer Step / 参数更新)
既然知道是谁的锅，那就改！
教练说：“下次手往右撇一点，肩膀沉一点。”
*   **AI 的行为**: 更新权重 (Weight Update)。
    *   `New_Weight = Old_Weight - Learning_Rate * Gradient`
    *   **学习率 (Learning Rate)** 就是“改多大”。如果改太大（步子迈太大），下次可能就偏右了；如果改太小，就要练很久才能中靶。

---

## 2. 梯度的物理意义：下山

如果把 Loss (误差) 想象成一座山的高度。我们的目标是**下山**（让误差最小化）。

*   **当前权重**: 你站在山腰的某个位置。
*   **梯度 (Gradient)**: 就是你脚下的**坡度**。它告诉你：“往这边走是上坡，往那边走是下坡。”
*   **反向传播**: 就是测量这个坡度的过程。
*   **更新**: 就是沿着下坡的方向走一步。

只要我们重复千万次：**测量坡度 -> 走一步 -> 测量坡度 -> 走一步**，最终我们就能走到山谷底（Loss 最小的地方），也就是 AI 练成神功的时候。

---

## 3. PPO 算法中的特殊性

在 PPO (Proximal Policy Optimization) 算法中，我们的“教练”稍微复杂一点。

我们有两个 loss 需要同时优化：

### A. 策略损失 (Policy Loss) —— 教练的鼓励与批评
*   **情景**: AI 刚才放了个大招，抢到了人头。
*   **Advantage (优势值)**: 正数 (+)。
*   **反向传播**: 告诉相关的神经元，“刚才那个动作是对的！以后看到这画面，**加大**放大招的概率！”

*   **情景**: AI 刚才冲进塔里送了。
*   **Advantage (优势值)**: 负数 (-)。
*   **反向传播**: 告诉相关的神经元，“刚才那个动作是傻X！以后看到这画面，**减小**冲塔的概率！”

### B. 价值损失 (Value Loss) —— 预言家的自我修养
*   **Critic (评论家)** 之前预测说：“这波能赢，胜率 80%。”
*   **结果**: 真的赢了。
*   **反向传播**: 很好，保持这个判断标准。

*   **Critic** 之前预测说：“这波能赢。”
*   **结果**: 输了。
*   **反向传播**: 你的判断标准有问题！下次这种局势，记得把分打低点！

---

## 4. 代码对应

在 `main/algo/ppo.py` 中，这神奇的一切浓缩在寥寥几行代码里：

```python
# 1. 报靶：计算总误差
loss = actor_loss + 0.5 * critic_loss

# 2. 准备：清空之前的记录（因为每次射击都是独立的）
self.optimizer.zero_grad()

# 3. 找原因：反向传播，计算梯度
loss.backward()

# 4. 微调：根据梯度修改所有神经元的权重
self.optimizer.step()
```

这就是**变强的本质**。
不需要上帝视角，不需要复杂的规则库，只需要**试错、反馈、微调**。
千万次的微调，造就了超越人类的直觉。
