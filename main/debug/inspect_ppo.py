import torch
import torch.nn as nn
import sys
import os

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
# è·å–é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(os.path.dirname(current_dir))

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
if project_root not in sys.path:
    sys.path.append(project_root)

from main.model import ActorCritic
from main.algo.ppo import PPO

def inspect_ppo_update():
    print("==================================================")
    print("       ğŸ§¬ PPO è¿›åŒ–å®éªŒå®¤ (Evolution Lab)       ")
    print("==================================================")
    
    # 1. å‡†å¤‡å®éªŒç¯å¢ƒ
    # æ¨¡æ‹Ÿä¸€ä¸ªå°è§„æ¨¡çš„æ¨¡å‹å’Œæ•°æ®
    action_dim = 4
    model = ActorCritic(state_dim=(3, 224, 224), action_dim=action_dim)
    
    # åˆ›å»º PPO ç®—æ³•å®ä¾‹
    ppo = PPO(model, lr=0.01, ppo_epochs=1, batch_size=2) # ä½¿ç”¨è¾ƒå¤§çš„ lr ä»¥ä¾¿è§‚å¯Ÿå˜åŒ–
    
    print("1. [åˆå§‹çŠ¶æ€] æ¨¡å‹å·²å°±ä½")
    print("   - æˆ‘ä»¬å‡è®¾æ¨¡å‹åœ¨æŸæ¬¡å›¢æˆ˜ä¸­ï¼Œèƒ¡ä¹±æ”¾äº†ä¸€ä¸ªæŠ€èƒ½ã€‚")
    print("   - æ­¤æ—¶ Critic ç»™å‡ºçš„è¯„åˆ† (Value) ä¹Ÿè®¸å¾ˆä½ã€‚")
    print("-" * 50)
    
    # 2. ä¼ªé€ ä¸€æ®µâ€œç»éªŒâ€ (Rollout Data)
    # å‡è®¾ Batch Size = 2
    fake_states = torch.randn(2, 3, 224, 224)
    fake_actions = torch.randn(2, 4)
    fake_log_probs = torch.tensor([-1.0, -1.0]) # å‡è®¾æ—§çš„æ¦‚ç‡
    fake_returns = torch.tensor([1.0, 0.5])     # å®é™…å›æŠ¥ (Reward): ä¸€ä¸ªå¥½(1.0)ï¼Œä¸€ä¸ªä¸€èˆ¬(0.5)
    fake_advantages = torch.tensor([0.5, -0.2]) # ä¼˜åŠ¿å€¼: ç¬¬ä¸€ä¸ªåŠ¨ä½œæ¯”å¹³å‡å¥½ï¼Œç¬¬äºŒä¸ªæ¯”å¹³å‡å·®
    fake_values = torch.tensor([0.5, 0.7])      # Critic ä¹‹å‰çš„é¢„æµ‹
    
    rollouts = {
        'states': fake_states,
        'actions': fake_actions,
        'log_probs': fake_log_probs,
        'returns': fake_returns,
        'advantages': fake_advantages,
        'values': fake_values
    }
    
    print(f"2. [æ”¶é›†ç»éªŒ] AI å›å¿†åˆšæ‰çš„æ“ä½œ")
    print(f"   - åŠ¨ä½œ1: ä¼˜åŠ¿å€¼ = {fake_advantages[0]:.1f} (åšå¾—å¥½ï¼åº”é¼“åŠ±)")
    print(f"   - åŠ¨ä½œ2: ä¼˜åŠ¿å€¼ = {fake_advantages[1]:.1f} (åšå¾—å·®ï¼åº”æƒ©ç½š)")
    print("-" * 50)
    
    # 3. è®°å½•æ›´æ–°å‰çš„æƒé‡ (ç”¨äºå¯¹æ¯”)
    # æˆ‘ä»¬åªçœ‹ Actor æŸä¸€ä¸ªæƒé‡çš„å˜åŒ–
    before_weight = model.actor_mean.weight.data[0][0].item()
    print(f"3. [è¿›åŒ–å‰] è§‚å¯ŸæŸä¸€ä¸ªç¥ç»å…ƒçªè§¦")
    print(f"   - æƒé‡å€¼: {before_weight:.6f}")
    
    # 4. æ‰§è¡Œ PPO æ›´æ–°
    print("\nâš¡ æ­£åœ¨è¿›è¡Œ PPO æ ¸å¿ƒæ›´æ–° (åå‘ä¼ æ’­)...")
    ppo.update(rollouts)
    
    # 5. è®°å½•æ›´æ–°åçš„æƒé‡
    after_weight = model.actor_mean.weight.data[0][0].item()
    
    print(f"\n4. [è¿›åŒ–å] ç¥ç»å…ƒçªè§¦å‘ç”Ÿäº†æ”¹å˜")
    print(f"   - æƒé‡å€¼: {after_weight:.6f}")
    print(f"   - å˜åŒ–é‡: {after_weight - before_weight:.6f}")
    print("-" * 50)
    
    print("\nâœ… æ¼”ç¤ºç»“æŸï¼")
    print("è¿™å°±æ˜¯â€œå¼ºåŒ–å­¦ä¹ â€çš„æœ¬è´¨ï¼š")
    print("AI æ ¹æ®ä¼˜åŠ¿å€¼ (Advantage)ï¼Œå¾®è°ƒæ¯ä¸€ä¸ªç¥ç»å…ƒçš„è¿æ¥æƒé‡ï¼Œ")
    print("è®©â€œå¥½åŠ¨ä½œâ€åœ¨æœªæ¥å‡ºç°çš„æ¦‚ç‡å˜å¤§ï¼Œè®©â€œååŠ¨ä½œâ€å‡ºç°çš„æ¦‚ç‡å˜å°ã€‚")
    print("-" * 50)
    print("ğŸ‘‰ æƒ³äº†è§£èƒŒåçš„æ•°å­¦åŸç†ï¼ˆæ¢¯åº¦ã€åå‘ä¼ æ’­ï¼‰ï¼Ÿ")
    print("   è¯·é˜…è¯»æ–‡æ¡£: docs/07_backpropagation_essence.md")

if __name__ == "__main__":
    inspect_ppo_update()
